{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "868d5b53-49ca-45a8-8b66-baccaffbffab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67d506d660f54a40a8cca10e069d32e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/131 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_8732\\2277307358.py:13: DeprecationWarning: torch.ao.quantization is deprecated and will be removed in 2.10. \n",
      "For migrations of users: \n",
      "1. Eager mode quantization (torch.ao.quantization.quantize, torch.ao.quantization.quantize_dynamic), please migrate to use torchao eager mode quantize_ API instead \n",
      "2. FX graph mode quantization (torch.ao.quantization.quantize_fx.prepare_fx,torch.ao.quantization.quantize_fx.convert_fx, please migrate to use torchao pt2e quantization API instead (prepare_pt2e, convert_pt2e) \n",
      "3. pt2e quantization has been migrated to torchao (https://github.com/pytorch/ao/tree/main/torchao/quantization/pt2e) \n",
      "see https://github.com/pytorch/ao/issues/2259 for more details\n",
      "  model = torch.quantization.quantize_dynamic(model, {torch.nn.Linear}, dtype=torch.qint8)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app '__main__'\n",
      " * Debug mode: on\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\n",
      " * Running on http://127.0.0.1:5000\n",
      "Press CTRL+C to quit\n",
      "127.0.0.1 - - [06/Feb/2026 09:09:23] \"GET / HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [06/Feb/2026 09:09:25] \"GET /static/images/ai-cartoon1-removebg-preview.png HTTP/1.1\" 304 -\n",
      "127.0.0.1 - - [06/Feb/2026 09:09:25] \"GET /static/bootstrap/css/bootstrap.min.css HTTP/1.1\" 304 -\n",
      "127.0.0.1 - - [06/Feb/2026 09:09:27] \"GET /static/images/T.png HTTP/1.1\" 304 -\n",
      "127.0.0.1 - - [06/Feb/2026 09:09:27] \"GET /favicon.ico HTTP/1.1\" 404 -\n",
      "127.0.0.1 - - [06/Feb/2026 09:09:30] \"GET /generator HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [06/Feb/2026 09:09:30] \"GET /static/bootstrap/css/bootstrap.min.css HTTP/1.1\" 304 -\n",
      "127.0.0.1 - - [06/Feb/2026 09:09:30] \"GET /static/images/T.png HTTP/1.1\" 304 -\n",
      "127.0.0.1 - - [06/Feb/2026 09:10:28] \"POST /api/generate HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [06/Feb/2026 09:11:46] \"POST /api/generate HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [06/Feb/2026 09:14:53] \"GET /generator HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [06/Feb/2026 09:14:53] \"GET /static/bootstrap/css/bootstrap.min.css HTTP/1.1\" 304 -\n",
      "127.0.0.1 - - [06/Feb/2026 09:14:53] \"GET /static/images/T.png HTTP/1.1\" 304 -\n",
      "127.0.0.1 - - [06/Feb/2026 09:14:58] \"POST /api/generate HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [06/Feb/2026 09:16:01] \"GET /generator HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [06/Feb/2026 09:16:01] \"GET /static/bootstrap/css/bootstrap.min.css HTTP/1.1\" 304 -\n",
      "127.0.0.1 - - [06/Feb/2026 09:16:01] \"GET /static/images/T.png HTTP/1.1\" 304 -\n",
      "127.0.0.1 - - [06/Feb/2026 09:16:07] \"POST /api/generate HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [06/Feb/2026 09:16:44] \"POST /api/generate HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [06/Feb/2026 09:17:06] \"GET / HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [06/Feb/2026 09:17:06] \"GET /static/bootstrap/css/bootstrap.min.css HTTP/1.1\" 304 -\n",
      "127.0.0.1 - - [06/Feb/2026 09:17:06] \"GET /static/images/ai-cartoon1-removebg-preview.png HTTP/1.1\" 304 -\n",
      "127.0.0.1 - - [06/Feb/2026 09:17:06] \"GET /static/images/T.png HTTP/1.1\" 304 -\n"
     ]
    }
   ],
   "source": [
    "from flask import Flask, render_template, request, jsonify\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "import torch\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "# Load Model once when the server starts\n",
    "model_name = \"TusharJoshi89/title-generator\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "\n",
    "# Quantize for CPU speed\n",
    "model = torch.quantization.quantize_dynamic(model, {torch.nn.Linear}, dtype=torch.qint8)\n",
    "\n",
    "@app.route('/')\n",
    "def index():\n",
    "    # Renders your landing page\n",
    "    return render_template('index.html')\n",
    "\n",
    "@app.route('/generator')\n",
    "def generator_page():\n",
    "    # Renders the actual generation tool page\n",
    "    return render_template('generator.html')\n",
    "\n",
    "@app.route('/api/generate', methods=['POST'])\n",
    "def generate():\n",
    "    data = request.json\n",
    "    abstract = data.get('abstract', '')\n",
    "    \n",
    "    if not abstract.strip():\n",
    "        return jsonify({\"error\": \"Abstract is empty\"}), 400\n",
    "\n",
    "    input_text = \"summarize: \" + abstract\n",
    "    inputs = tokenizer.encode(input_text, return_tensors=\"pt\", max_length=512, truncation=True)\n",
    "\n",
    "    outputs = model.generate(\n",
    "        inputs, \n",
    "        max_length=40, \n",
    "        num_beams=5, \n",
    "        num_return_sequences=3, \n",
    "        early_stopping=True\n",
    "    )\n",
    "\n",
    "    titles = [tokenizer.decode(out, skip_special_tokens=True).title() for out in outputs]\n",
    "    return jsonify({\"titles\": titles})\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Setting use_reloader=False prevents the 'SystemExit' error in Jupyter\n",
    "    app.run(host='127.0.0.1', port=5000, debug=True, use_reloader=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e50795e-3fe5-4c76-82dc-263c91d16736",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4360962a-7476-45e7-9aa5-afa4b6a1cf96",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d73b1c7d-a4ef-442a-9656-90605fc5c037",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
